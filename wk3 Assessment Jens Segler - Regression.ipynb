{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih3TrpA0rzvc"
   },
   "source": [
    "# Assessment: Weekly deliverable template - Regression\n",
    "<font color='#AC135A'>**Applied Machine Learning** - **Sheffield Hallam University**</font>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "Add your solutions to the different sub-tasks requested for this deliverable.\n",
    "\n",
    "<font color='red'>DO NOT ADD MORE CELLS TO THE FINAL VERSION OF THIS NOTEBOOK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2GxruY5O0A2"
   },
   "source": [
    "### 1. Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAGVCU0gqFwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rating   num_reviews        price         body      acidity\n",
      "count  1775.000000   1775.000000  1775.000000  1775.000000  1775.000000\n",
      "mean      4.411887    538.023662   148.786017     4.258592     2.921690\n",
      "std       0.150982   1352.363601   289.401877     0.652938     0.334181\n",
      "min       4.200000     25.000000     4.990000     2.000000     1.000000\n",
      "25%       4.300000     56.000000    33.015000     4.000000     3.000000\n",
      "50%       4.400000    134.000000    58.260000     4.000000     3.000000\n",
      "75%       4.500000    499.000000   125.000000     5.000000     3.000000\n",
      "max       4.900000  32624.000000  3119.080000     5.000000     3.000000\n",
      "Unique values in 'winery': 425\n",
      "Unique values in 'wine': 738\n",
      "Unique values in 'year': 71\n",
      "Unique values in 'country': 1\n",
      "Unique values in 'region': 66\n",
      "Unique values in 'type': 21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('wines_SPA.csv')\n",
    "# I remove all rows with missing values and duplicates. \n",
    "# If there weren't a constraint on the number of coding cells, I would first analyse \n",
    "# the missing values and duplicates, and how many data they take away from the entire dataset\n",
    "# in order to decide whether to impute or drop them. \n",
    "# Obviously I did this outside of this coding cell when preparing for this task.\n",
    "# Also, I checked that the resulting distribution don't significantly change after dropping.\n",
    "# Having done all this work, I would now feel silly to pretend I don't know anything about the dataset.\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "print(df.describe())\n",
    "numerical_columns = df.describe().columns\n",
    "for column in df.columns:\n",
    "    if column not in numerical_columns:\n",
    "        print(f\"Unique values in '{column}': {len(df[column].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0kcDUa2O5W9"
   },
   "source": [
    "**Observations:**  \n",
    "\n",
    "\n",
    "After cleaning the data from missing values and after removing duplicates, the dataset still has 1,775 examples. \n",
    "\n",
    "*NUMERICAL ATTRIBUTES*\n",
    "\n",
    "*rating*\n",
    "\n",
    "The rating values have a range between 4.2 and 4.9. Removing the duplicates has dramatically improved the distribution of values. The 75% quartile was at 4.2 (the minimum!), and is now at 4.5. Adding/substracting the std to/from the mean shows that 68% of examples have a rating between 4.3 and 4.6, and 95% of examples have rating below 4.8. From these data I expect a reasonably good distribition at a location of 4.4, the median. For normalisation, a standard approach should do. Rating seems a classic label attribute, as businesses may want to predict customer rating based on a range of criteria, and indirectly (as a proxy label) sales success. \n",
    "\n",
    "*num_reviews*\n",
    "\n",
    "From the name I infer that this is the number of available reviews that lead to the \"rating\". The range is very high between 25 and 32.624. However, the 25% and 75% quartiles are at 56 and 499, repsectively. This means that 50% of examples have review numbers in this range. The location is skewed to lower values. The standard deviation is quite high though. Only 68% of examples will have review numbers below mean+std = 1890, and 95% are below mean+2*std = 3243. (I'm ignoring the bottom because the minimum value is higher than mean-std.) This suggest we definitely can see some outliers at the top end (32,624). Simply looking at the magnitudes of numbers I am thinking of a logarithmic curve. Possibly a logarithmic normalisation may be a good approach here. \n",
    "\n",
    "*price*\n",
    "\n",
    "We see similar behaviour of the price data: A huge range (4.99–3119.08), a relatively strong 25-75% range (33.015-125) that is located at the lower price range, and again a very high standard deviation that shows that only 68% of examples have a price below 438.19, while 95% have a price below 727.59. The expectation again is to have some outliers at the top end. Again for normalisation, a logarithmic approach may be indicated. As for correlation analysis it may be an interesting question if the price correlates to rating in some way. Also, the expectation is that the number of reviews is higher towards the lower end of the price range. On the other hand, a higher price may correlate with an older year. - I can see price as a target label for producers who have to find the sweet spot for their own product.\n",
    "\n",
    "*body*\n",
    "\n",
    "The range for body is between 2.0 and 5.0, and we don't see any decimals. This could mean this body is likely determined as a rating between 1 and 5. the 25% quartile is at 4. This means that the more than 75% of examples has a body value of 4 or 5, i.e. one of two values. This means the informational content of this attribute is questionable.\n",
    "\n",
    "*acidity*\n",
    "\n",
    "For acidity the case is even more extrem mean-2*std is at 2.25, so 95% of examples have as value 3 (the maximum value). This attribute we can ignore as there is no informational content. \n",
    "\n",
    "\n",
    "*CATEGORICAL ATTRIBUTES*\n",
    "\n",
    "*winery and wine*\n",
    "\n",
    "In order to have enough training examples I understand that the number of examples should be at least one magnitude higher than the number of classes. In a dataset of 1,775 that means that any attribute with more than 177 values may not be too helpful as there are simply not enough training data. \n",
    "In our dataset, that is true for 'winery' and 'wine'. Hence I will not use these attributes. \n",
    "\n",
    "*year*\n",
    "\n",
    "I know from previous work that the 'year' column has some missing values that are already imputed with 'N.V.'. I could also drop these, but I am careful as maybe there was a reason why no year was given, maybe these are blended wines. 'Year' is also an ordinal category because the expectation is that older wines are more expensive – comparisons are reasonable with 'year', but not addition etc, so 'year' is not a numerical attribute. To get floats, which are required for the vectors, I can directly convert the year to a number. Then I can impute the missing values with the median to give the attribute less weight for these examples. On top of it I can create an additional column 'year_missing' as 0/1. \n",
    "\n",
    "*country*\n",
    "\n",
    "There is only one value in this category, and hence there is no informational content. I am dropping this attribute. \n",
    "\n",
    "*region and type*\n",
    "\n",
    "These attributes have a reasonable number of unique values (66 and 21, respectively). It also seems from my domain knowledge that these attributes may well be relevant for predicting price and/or rating. So I'll keep them. They are still quite numerous for one-hot-encoding, so maybe frequency encoding is the way forward (https://letsdatascience.com/frequency-encoding/).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkJsPP6wPIaV"
   },
   "source": [
    "### 2. At least two exploratory graphs with captions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKpiKosDPXN9"
   },
   "outputs": [],
   "source": [
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLTgeF7cP0qn"
   },
   "source": [
    "**Observations:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmcmWCYOPXj9"
   },
   "source": [
    "### 3. Histogram showing rating imbalance (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsUJYXMpPgJd"
   },
   "outputs": [],
   "source": [
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmkwY1OKP1X_"
   },
   "source": [
    "**Observations:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQuTlNBrPgdV"
   },
   "source": [
    "### 4. Price-regression model + metrics + plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0S5I_o5rPkx8"
   },
   "outputs": [],
   "source": [
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADZHfhQmP2G1"
   },
   "source": [
    "**Observations:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl1AJn4nPlH0"
   },
   "source": [
    "### 5. Quality-regression model + metrics + plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UM9QBXIPnts"
   },
   "outputs": [],
   "source": [
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoQ0iRQ7P2o0"
   },
   "source": [
    "**Observations:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GlUHUaNPoF0"
   },
   "source": [
    "### 6. Over/under-fitting discussion and fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67XKmjB8Pq4c"
   },
   "outputs": [],
   "source": [
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEi87AZRP3UC"
   },
   "source": [
    "**Observations:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7odzWG7bPrNE"
   },
   "source": [
    "### 7. Apply cross-validation or statistical analyses on the features or results of the regression models (advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJ6rpHYmPxYc"
   },
   "outputs": [],
   "source": [
    "# ---- Your code here ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iInpxtArP3-V"
   },
   "source": [
    "**Observations:**  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2RmBRwLyyZTJP8vv8Z0si",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
