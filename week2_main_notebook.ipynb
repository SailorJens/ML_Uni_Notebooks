{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: An End-to-End Machine Learning System Development\n",
        "<font color='#AC135A'>**Applied Machine Learning** - **Sheffield Hallam University**</font>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "The task is for you to have a taste of developing a complete machine learning system from the beginning to the end. We will use open-source datasets for this week practice. In particular, we will explore the *iris dataset* which is a classic dataset in Machine Learning collacted by the botanist Edgar Anderson and made famous by the renowned statistician Ronald Fisher."
      ],
      "metadata": {
        "id": "WffwRKmHQ5AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"#672146\">Part I - The data</font>"
      ],
      "metadata": {
        "id": "ZYP9hlmSTTLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data\n",
        "\n",
        "First we need to load the data and check important aspects like:\n",
        "- Are there any missing or invalid values?\n",
        "- Are they all in acceptable ranges?\n",
        "- How are the labels encoded?\n"
      ],
      "metadata": {
        "id": "GYNDyRkgDYAB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzclOQVIO9vu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris() # Loads a dictionary\n",
        "\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "print(f\"Target names: {iris.target_names}\")\n",
        "print(f\"Feature names: {iris.feature_names}\")\n",
        "#print(iris.DESCR)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "df['target_name'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
        "\n",
        "print(df.isnull().sum()) # Missing values\n",
        "# Show the first few rows\n",
        "print(df.sample(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Exercise 1.1:* Replace the target encodings by the target names in the table.\n",
        "\n",
        "> *Exercise 1.2:* Generate the summary statistics on your own. Plot the class distribution using a bar plot."
      ],
      "metadata": {
        "id": "vjRBLgj-54KL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features and feature space"
      ],
      "metadata": {
        "id": "oT57NiwC7fXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can take a closer look to the features in our dataset. Let's start by plotting the histograms an explore the data distribution:"
      ],
      "metadata": {
        "id": "xc2Af4sYpxJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df[iris.feature_names].hist(bins=10,\n",
        "                            figsize=(10, 8),\n",
        "                            edgecolor='black')\n",
        "plt.suptitle(\"Histograms of Iris Features\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "676azH4TIF_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also start exploring the relationships between different features by using scatter plots:"
      ],
      "metadata": {
        "id": "UGpSGZBKp9n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(df['sepal length (cm)'], df['sepal width (cm)'], 'o')\n",
        "plt.xlabel('sepal length (cm)')\n",
        "plt.ylabel('sepal width (cm)')\n",
        "plt.show()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oY9REorgBVmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', hue='target_name')\n",
        "plt.title(\"Scatter Plot of Sepal Length vs. Sepal Width\")\n",
        "plt.xlabel(\"Sepal Length (cm)\")\n",
        "plt.ylabel(\"Sepal Width (cm)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UofBtOQZTIjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course it would be more useful if we could check the information about all the feature at the same time!"
      ],
      "metadata": {
        "id": "efAWwgzArKO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df['species'] = pd.Categorical.from_codes(iris.target,\n",
        "                                          iris.target_names)\n",
        "\n",
        "# Pairplot (scatterplot matrix)\n",
        "sns.pairplot(df.drop(columns=['target']),\n",
        "             hue='species', corner=True)\n",
        "plt.suptitle(\"Iris Dataset - Scatter Plot Matrix\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OUvxpJhw-oCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can summarise this information using the correlation matrix:"
      ],
      "metadata": {
        "id": "fk2poJQPrutT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df.drop(columns=['target', 'target_name', 'species']).corr() # correlation\n",
        "\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yda1B2A7r1bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it is worth taking another look at the feature histograms, but this time separated by the labels. What do you see?"
      ],
      "metadata": {
        "id": "L2Gt1VVvsGB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['blue', 'green', 'red']\n",
        "\n",
        "for feature in feature_names:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    for species, color in zip(target_names, colors):\n",
        "        subset = df[df['target_name'] == species]\n",
        "        sns.histplot(subset[feature],\n",
        "                     kde=False,\n",
        "                     label=species,\n",
        "                     color=color,\n",
        "                     stat='density',\n",
        "                     bins=15, element=\"step\")\n",
        "\n",
        "    plt.title(f'Histogram of {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "G88upmuMWzsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Exercise 1.3*: Compute the mean, variance, correlation coefficient for individual features."
      ],
      "metadata": {
        "id": "vBu5OLJ3aHxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualise important summary information about the distribution using box-plots. What information can you see?"
      ],
      "metadata": {
        "id": "k4MW8OGytQ6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define color palette for species\n",
        "colors = ['blue', 'green', 'red']\n",
        "\n",
        "# Create one box plot per feature\n",
        "for feature in iris.feature_names:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x='target_name', y=feature,\n",
        "                data=df,\n",
        "                hue = 'target_name',\n",
        "                legend=False)\n",
        "    plt.title(f'Box Plot of {feature} by Species')\n",
        "    plt.xlabel('Species')\n",
        "    plt.ylabel(feature)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "t7FUJMUE93wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"#672146\">Part II: The model</font>"
      ],
      "metadata": {
        "id": "_BC1G2zhTYOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the K-Nearest-Neighbours (KNN) for this task. It is our first classifier and it's easy to understand. It's use in this task will reveal the shared notation we will use for other classifiers in this module."
      ],
      "metadata": {
        "id": "Grksqp_gt1R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Split the DataFrame into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0)\n",
        "\n",
        "# Separate features and target for training and testing\n",
        "X_train_df = train_df[feature_names]\n",
        "y_train_df = train_df['target']\n",
        "X_test_df = test_df[feature_names]\n",
        "y_test_df = test_df['target']\n"
      ],
      "metadata": {
        "id": "-4Qe5Z2ZTfoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Exercise 2.1*: Plot the histograms of the train and test sets, do they look similar?"
      ],
      "metadata": {
        "id": "tDfp_SUqvvWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the training of the classifier is straightforward:"
      ],
      "metadata": {
        "id": "KgOuJnw8v3wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the KNN model\n",
        "knn_df = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_df.fit(X_train_df, y_train_df)\n",
        "\n",
        "# Make a prediction on the first test sample using the DataFrame\n",
        "sample_df = X_test_df.iloc[0:1]\n",
        "predicted_class_df = knn_df.predict(sample_df)[0]\n",
        "actual_class_df = y_test_df.iloc[0]\n",
        "\n",
        "print(\"Predicted Class: \", target_names[predicted_class_df])\n",
        "print(\"Actual Class: \", target_names[actual_class_df])"
      ],
      "metadata": {
        "id": "xHmatRFJvadf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess the performance of our classifier, we start by computing the accuracy score:"
      ],
      "metadata": {
        "id": "hvI8QQ8U9EVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred_df = knn_df.predict(X_test_df)\n",
        "accuracy = accuracy_score(y_test_df, y_pred_df)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "bh_ryBZQ9Lg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Exercise 2.2*: Add noise to the data, how does the accuracy changes?"
      ],
      "metadata": {
        "id": "SgVqKKn3wOfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also compute the confusion matrix as seen in the lecture:"
      ],
      "metadata": {
        "id": "KzhIbdrOxQky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test_df, y_pred_df)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\",\n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(f\"Confusion Matrix (Accuracy: {accuracy:.2f})\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NOlfkSVvxVmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can get a report-style summary of the performance of our model:"
      ],
      "metadata": {
        "id": "JONn7ZebyJCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test_df, y_pred_df, target_names=iris.target_names))"
      ],
      "metadata": {
        "id": "pid-g-vNxY3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color=\"#672146\">Part III: Improving the model's performace</font>"
      ],
      "metadata": {
        "id": "qyfTUUkhsT7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you will try to improve the model's performance by implementing some commonly used techniques.\n",
        "\n",
        "\n",
        "> *Exercise 2.3 Feature scaling*: KNN is sensitive to the scale of features because it uses distance metrics. Study the effect of scaling in the different features of the dataset.\n",
        "\n",
        "To do so, apply StandardScaler or MinMaxScaler from sklearn.preprocessing. For example:\n",
        "\n",
        "```\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_df)\n",
        "X_test_scaled = scaler.transform(X_test_df)\n",
        "```\n",
        "Plot the original features and the scaled ones. Retrain the knn classifier with different scalings. Does performance improve?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9tod_v7euS7A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecp5zSL1shG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Exercise 2.4 Hyper-parameter search*: Try to answer the following question: What is the best number of neighbours $k$.\n",
        "\n",
        "To do so, use the function:\n",
        "\n",
        "```\n",
        "scores = cross_val_score(knn, X, y, cv=5)\n",
        "```\n",
        "\n",
        "Read the documentation, import the necessary functions, and repeat the cross validation for different values of $k$. What is the best? Why is cross-validation important?\n",
        "\n"
      ],
      "metadata": {
        "id": "2dHRVEhb4dq4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4MQLEbF4fmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Exercise 2.4 Using different metrics*: Distance choice affects how \"closeness\" is defined. Study the effect of different distance measures on the performance.\n",
        "\n",
        "To do so, ry 'manhattan', 'minkowski', or 'cosine' distances. Show the confusion matrix for each choice.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "```\n",
        "KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "N6Bn5qPQ4f65"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k5olSkTz4jUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Exercise 2.5 Feature selection*: Remove one feature at a time. Does performance improve? Try using only two highly correlated features."
      ],
      "metadata": {
        "id": "nlWuza_54j0v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVD54rvx4lK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}