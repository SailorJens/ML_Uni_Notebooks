{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e22a341",
      "metadata": {
        "id": "8e22a341"
      },
      "source": [
        "# Week 3: Supervised Machine Learning I : Regression\n",
        "<font color='#AC135A'>**Applied Machine Learning** - **Sheffield Hallam University**</font>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook contains exercises spanning data quality, matrix manipulation, regression modeling, and evaluation techniques. Go through all of them and experiment with variations and the tasks asked with the given code.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 1: Good Data vs. Bad Data\n",
        "\n",
        "### Exercise 1: Plotting Simple Lines\n",
        "\n",
        "**Explanation:** Visualize how slope and intercept shape a line. By plotting multiple lines, identify which ones represent clear trends and which could be misleading if noisy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2777d79f",
      "metadata": {
        "id": "2777d79f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define x-range\n",
        "x = np.linspace(0, 10, 100)\n",
        "\n",
        "# Three lines with varied parameters\n",
        "y1 = 0.5 * x + 2     # Mild incline\n",
        "y2 = 2.0 * x - 1     # Steeper incline\n",
        "y3 = -1.0 * x + 8    # Negative incline\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x, y1, label='0.5x + 2')\n",
        "plt.plot(x, y2, label='2.0x - 1')\n",
        "plt.plot(x, y3, label='-1.0x + 8')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Effect of Slope and Intercept')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2049360",
      "metadata": {
        "id": "b2049360"
      },
      "source": [
        "\n",
        "**Task:** Describe which line would be easiest to fit in the presence of noise and why. Change the paramenters and observe the result.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 2: Noisy Linear Data\n",
        "\n",
        "**Explanation:** Real-world data often includes noise. Generate a true linear relationship and add Gaussian noise to observe how noise level affects model fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a8d0bd",
      "metadata": {
        "id": "f6a8d0bd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "true_slope, true_intercept = 2.5, -1.0\n",
        "x = np.linspace(0, 10, 50)\n",
        "\n",
        "# Generate clean and noisy targets\n",
        "y_clean = true_slope * x + true_intercept\n",
        "noise = np.random.normal(loc=0, scale=2.0, size=x.shape) # Noise\n",
        "y_noisy = y_clean + noise\n",
        "\n",
        "plt.scatter(x, y_noisy, alpha=0.7, label='Noisy data')\n",
        "plt.plot(x, y_clean, 'r-', lw=2, label='True line')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Data with Noise')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7dec294",
      "metadata": {
        "id": "d7dec294"
      },
      "source": [
        "\n",
        "**Task:** Modify the `scale` of the noise distribution; note at which point the noise obscures the true trend.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 3: Manual 3D Plane\n",
        "\n",
        "**Explanation:** Extend linear models to two predictors. In this case, the model represents a plane in 3D. Plot the plane defined by $z = w_1x + w_2y + b$ to build intuition for multivariate regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6a8a66",
      "metadata": {
        "id": "eb6a8a66"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "w1, w2, b = 1.0, -0.5, 2.0\n",
        "x = np.linspace(-5, 5, 20)\n",
        "y = np.linspace(-5, 5, 20)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = w1 * X + w2 * Y + b\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, alpha=0.6)\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "plt.title('3D Plane Visualization')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e646359",
      "metadata": {
        "id": "2e646359"
      },
      "source": [
        "\n",
        "**Task:** Change `w1`, `w2`, and `b` to see how the plane shifts and tilts.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 2: Matrix Manipulation with NumPy & Pandas\n",
        "\n",
        "### Exercise 4: NumPy Means\n",
        "\n",
        "**Explanation:** Practice basic array operations by computing means manually, reinforcing understanding of vectorized operations versus explicit loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0e8922",
      "metadata": {
        "id": "2a0e8922"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.random.randn(5, 3)  # 5 rows, 3 columns\n",
        "row_means = np.mean(data, axis=1)\n",
        "col_means = np.mean(data, axis=0)\n",
        "print('Row means:', row_means)\n",
        "print('Column means:', col_means)\n",
        "\n",
        "# Verify by summing and dividing\n",
        "row_means_manual = np.sum(data, axis=1) / data.shape[1]\n",
        "col_means_manual = np.sum(data, axis=0) / data.shape[0]\n",
        "print('Verified row means:', row_means_manual)\n",
        "print('Verified col means:', col_means_manual)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6b335a7",
      "metadata": {
        "id": "e6b335a7"
      },
      "source": [
        "\n",
        "**Task:** Without using `np.mean`, compute the means through sums (with a for loop) and division and compare.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 5: Pandas DataFrame Basics\n",
        "\n",
        "**Explanation:** Load tabular data and inspect its structure. Summary statistics help identify scale, range, and potential anomalies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c0c8cd",
      "metadata": {
        "id": "88c0c8cd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Load data\n",
        "# You can choose any dataset from kaggle to experiment\n",
        "# df = pd.read_csv('data.csv')\n",
        "# For demonstration, we generate a synthetic dataset with random features:\n",
        "np.random.seed(1)\n",
        "df = pd.DataFrame({\n",
        "    'A': np.random.randn(100),\n",
        "    'B': np.random.randn(100) * 2 + 5,\n",
        "    'C': np.random.randint(0, 50, 100)\n",
        "})\n",
        "\n",
        "print(df.head())\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b5c547",
      "metadata": {
        "id": "13b5c547"
      },
      "source": [
        "\n",
        "**Task:** Add a new column `D = 2*A - 0.5*B` and display the updated `df.head()`.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 6: DataFrame ↔ NumPy Conversion\n",
        "\n",
        "**Explanation:** Convert between Pandas and NumPy to leverage both libraries’ strengths. Perform matrix multiplication on DataFrame values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce3b967f",
      "metadata": {
        "id": "ce3b967f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Assume df from previous exercise\n",
        "data = df[['A', 'B', 'C']].values\n",
        "\n",
        "# Define transformation matrix\n",
        "T = np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1]])\n",
        "\n",
        "# Apply transformation\n",
        "result = data.dot(T)\n",
        "new_df = pd.DataFrame(result, columns=['A2', 'B2', 'C2'])\n",
        "print(new_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a3f78e",
      "metadata": {
        "id": "38a3f78e"
      },
      "source": [
        "\n",
        "**Task:** Explain the difference between row-wise and column-wise operations in Pandas vs NumPy.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 3: Linear Regression with Synthetic Data\n",
        "\n",
        "### Exercise 7: Informative vs. Noise Feature\n",
        "\n",
        "**Explanation:** Create a dataset with one meaningful feature and one random noise feature to see how regression identifies signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d11577",
      "metadata": {
        "id": "c3d11577"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "x1 = rng.randn(100)\n",
        "x2 = rng.randn(100)\n",
        "y = 3 * x1 + rng.randn(100) * 0.5 # x1 is the informative feature\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "model = LinearRegression().fit(X, y)\n",
        "print('Coefficients:', model.coef_)\n",
        "print('Intercept:', model.intercept_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e597cf",
      "metadata": {
        "id": "f6e597cf"
      },
      "source": [
        "\n",
        "**Task:** Identify which coefficient corresponds to the informative feature. Discuss why the noise feature’s coefficient is near zero.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 8: Handling Missing Data\n",
        "\n",
        "**Explanation:** Missing values are common. Use [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) to fill gaps and evaluate impact on model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869f5da4",
      "metadata": {
        "id": "869f5da4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create missing values\n",
        "X_missing = X.copy() # copy the previous data\n",
        "mask = rng.rand(*X_missing.shape) < 0.1\n",
        "X_missing[mask] = np.nan\n",
        "\n",
        "# Impute\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_filled = imputer.fit_transform(X_missing)\n",
        "\n",
        "# Compare performance\n",
        "def fit_score(Xi):\n",
        "    m = LinearRegression().fit(Xi, y)\n",
        "    return r2_score(y, m.predict(Xi))\n",
        "\n",
        "print('R² before imputation:', fit_score(np.nan_to_num(X_missing)))\n",
        "print('R² after imputation:', fit_score(X_filled))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a170a2",
      "metadata": {
        "id": "d9a170a2"
      },
      "source": [
        "\n",
        "**Task:** Experiment with different imputation strategies (`median`, `most_frequent`) and compare R².\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 9: Impact of Outliers\n",
        "\n",
        "**Explanation:** Extreme values can skew regression. Introduce outliers and visualize their effect on fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be83941d",
      "metadata": {
        "id": "be83941d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "y_out = y.copy()\n",
        "outlier_indices = rng.choice(len(y_out), size=5, replace=False)\n",
        "y_out[outlier_indices] += rng.randn(5) * 20\n",
        "\n",
        "model_std = LinearRegression().fit(X, y)\n",
        "model_out = LinearRegression().fit(X, y_out)\n",
        "\n",
        "print('R² without outliers:', model_std.score(X, y))\n",
        "print('R² with outliers:', model_out.score(X, y_out))\n",
        "\n",
        "# Residual plot\n",
        "plt.scatter(model_out.predict(X), model_out.predict(X) - y_out)\n",
        "plt.axhline(0, color='r')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Residual')\n",
        "plt.title('Residuals with Outliers')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f72451",
      "metadata": {
        "id": "f6f72451"
      },
      "source": [
        "\n",
        "**Task:** Describe how outliers affect coefficient estimates and residual distribution.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 10: Train/Test Split\n",
        "\n",
        "**Explanation:** Evaluate generalization by comparing training vs. test performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efde44e8",
      "metadata": {
        "id": "efde44e8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "model = LinearRegression().fit(X_train, y_train)\n",
        "print('Train R²:', model.score(X_train, y_train))\n",
        "print('Test R²:', model.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c37017d",
      "metadata": {
        "id": "1c37017d"
      },
      "source": [
        "\n",
        "**Task:** Based on R² scores, assess if the model overfits or underfits.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 11: Normal Equation\n",
        "\n",
        "**Explanation:** (advanced) Use closed-form solution for linear regression model $\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta$ using matrix algebra:\n",
        "\n",
        "$\n",
        "\\hat{\\boldsymbol\\beta}\n",
        "= \\bigl(\\mathbf{X}^\\top \\mathbf{X}\\bigr)^{-1}\\,\\mathbf{X}^\\top\\,\\mathbf{y}.\n",
        "$\n",
        "\n",
        "$\\hat{\\boldsymbol\\beta}$ is the estimate of our parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faba0cc1",
      "metadata": {
        "id": "faba0cc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Add bias term\n",
        "X_b = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "# Compute theta\n",
        "beta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "print('Manual parameters:', beta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9995a5",
      "metadata": {
        "id": "9f9995a5"
      },
      "source": [
        "\n",
        "**Task:** Compare manual `theta` to `sklearn` coefficients (including intercept).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 12: 3D Regression Plane\n",
        "\n",
        "**Explanation:** Visualize fitted plane for two features to inspect model fit in 3D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe5de89",
      "metadata": {
        "id": "bfe5de89"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression().fit(X, y)\n",
        "w, b0 = model.coef_, model.intercept_\n",
        "\n",
        "# Create grid\n",
        "grid_x, grid_y = np.meshgrid(np.linspace(X[:,0].min(), X[:,0].max(), 20),\n",
        "                             np.linspace(X[:,1].min(), X[:,1].max(), 20))\n",
        "grid_z = w[0]*grid_x + w[1]*grid_y + b0\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X[:,0], X[:,1], y, alpha=0.6)\n",
        "ax.plot_surface(grid_x, grid_y, grid_z, alpha=0.4)\n",
        "ax.set_xlabel('x1')\n",
        "ax.set_ylabel('x2')\n",
        "ax.set_zlabel('y')\n",
        "plt.title('Fitted Regression Plane')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748e09d5",
      "metadata": {
        "id": "748e09d5"
      },
      "source": [
        "\n",
        "**Task:** Rotate the plot interactively to inspect fit quality.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 4: Gradient Descent\n",
        "\n",
        "### Exercise 13: Batch Gradient Descent\n",
        "\n",
        "**Explanation:** Implement gradient descent to minimize MSE for univariate regression, and observe parameter convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a2cbec2",
      "metadata": {
        "id": "8a2cbec2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "t = rng\n",
        "# x = ? - Create some random feature with the right dimensions\n",
        "y = 2 * x + 1 + rng.randn(100) * 0.5\n",
        "# Initialize parameters\n",
        "w, b = 0.0, 0.0\n",
        "lr = 0.01 # Learning rate\n",
        "iters = 100\n",
        "w_list, b_list = [], []\n",
        "\n",
        "for i in range(iters):\n",
        "    y_pred = w * x + b\n",
        "    # Compute derivative\n",
        "    dw = (-2/len(x)) * np.dot(x, (y - y_pred))\n",
        "    db = (-2/len(x)) * np.sum(y - y_pred)\n",
        "    # Perform gradient descent\n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "    # Keep track of the evolution\n",
        "    w_list.append(w)\n",
        "    b_list.append(b)\n",
        "\n",
        "# Add a plot of the true values as a horizontal line\n",
        "plt.plot(w_list, label='w')\n",
        "plt.plot(b_list, label='b')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Parameter value')\n",
        "plt.legend()\n",
        "plt.title('Gradient Descent Convergence')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1354241a",
      "metadata": {
        "id": "1354241a"
      },
      "source": [
        "\n",
        "**Task:** Experiment with different learning rates and iteration counts.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 14: Cost Surface & Path\n",
        "\n",
        "**Explanation:** Visualize MSE cost surface over a grid of `(w,b)` and overlay gradient descent trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a4424d",
      "metadata": {
        "id": "51a4424d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute cost surface\n",
        "grid_w = np.linspace(-1, 5, 50)\n",
        "grid_b = np.linspace(-3, 5, 50)\n",
        "W, B = np.meshgrid(grid_w, grid_b)\n",
        "J = np.zeros_like(W)\n",
        "for i in range(W.shape[0]):\n",
        "    for j in range(W.shape[1]):\n",
        "        y_pred = W[i,j]*x + B[i,j]\n",
        "        J[i,j] = np.mean((y - y_pred)**2)\n",
        "\n",
        "# Contour plot\n",
        "plt.contour(W, B, J, levels=30)\n",
        "# Overlay path from previous exercise\n",
        "plt.plot(w_list, b_list, 'ro-')\n",
        "plt.xlabel('w')\n",
        "plt.ylabel('b')\n",
        "plt.title('Cost Surface with Gradient Path')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f7ca81",
      "metadata": {
        "id": "30f7ca81"
      },
      "source": [
        "\n",
        "**Task:** Try different starting points or learning rates and compare paths.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 15: Stochastic & Mini-Batch GD\n",
        "\n",
        "**Explanation:** (Advanced) Compare full-batch, stochastic (single sample), and mini-batch gradient descent, visualizing convergence speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b10f0b8",
      "metadata": {
        "id": "7b10f0b8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sgd(X, y, lr, iters):\n",
        "    w, b = 0, 0\n",
        "    costs = []\n",
        "    for epoch in range(iters):\n",
        "        for xi, yi in zip(X, y):\n",
        "            dw = -2 * xi * (yi - (w*xi + b))\n",
        "            db = -2 * (yi - (w*xi + b))\n",
        "            w -= lr * dw\n",
        "            b -= lr * db\n",
        "        costs.append(np.mean((y - (w*X + b))**2))\n",
        "    return costs\n",
        "\n",
        "cost_sgd = sgd(x, y, 0.01, 50)\n",
        "cost_batch = []  # reuse J from batch exercise or recompute\n",
        "\n",
        "plt.plot(cost_sgd, label='SGD')\n",
        "# plt.plot(cost_batch, label='Batch GD')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE')\n",
        "plt.legend()\n",
        "plt.title('SGD vs Batch GD')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f49fc03",
      "metadata": {
        "id": "7f49fc03"
      },
      "source": [
        "\n",
        "**Task:** Implement mini-batch version and add to the comparison plot.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 5: Non-Linear Regression, Overfitting & Underfitting\n",
        "\n",
        "### Exercise 16: Polynomial Regression\n",
        "\n",
        "**Explanation:** Fit polynomial models of varying degree to quadratic data to observe model flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30618a8d",
      "metadata": {
        "id": "30618a8d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Data\n",
        "t = rng\n",
        "x_poly = np.linspace(-3, 3, 100).reshape(-1,1)\n",
        "y_poly = 1 - 2*x_poly + x_poly**2 + rng.randn(100,1)*5\n",
        "\n",
        "degrees = [1, 2, 10]\n",
        "for d in degrees:\n",
        "    poly = PolynomialFeatures(d)\n",
        "    Xd = poly.fit_transform(x_poly)\n",
        "    model = LinearRegression().fit(Xd, y_poly)\n",
        "    y_pred = model.predict(Xd)\n",
        "    plt.plot(x_poly, y_pred, label=f'degree={d}')\n",
        "\n",
        "plt.scatter(x_poly, y_poly, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.title('Polynomial Regression Fits')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752182bf",
      "metadata": {
        "id": "752182bf"
      },
      "source": [
        "\n",
        "**Task:** Compute and compare R² for each degree.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 17: Underfitting vs Overfitting\n",
        "\n",
        "**Explanation:** Fit a constant model and a high-degree polynomial to highlight under- and overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0a4c1a",
      "metadata": {
        "id": "ae0a4c1a"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(x_poly, y_poly, test_size=0.3, random_state=0)\n",
        "\n",
        "# Underfit: constant\n",
        "dummy = DummyRegressor(strategy='mean').fit(Xtr, ytr)\n",
        "# Overfit: degree 15\n",
        "poly15 = PolynomialFeatures(15)\n",
        "X15 = poly15.fit_transform(Xtr)\n",
        "model15 = LinearRegression().fit(X15, ytr)\n",
        "\n",
        "# Evaluate\n",
        "print('Dummy train MSE:', mean_squared_error(ytr, dummy.predict(Xtr)))\n",
        "print('Dummy test MSE:', mean_squared_error(yte, dummy.predict(Xte)))\n",
        "print('Poly15 train MSE:', mean_squared_error(ytr, model15.predict(X15)))\n",
        "X15_te = poly15.transform(Xte)\n",
        "print('Poly15 test MSE:', mean_squared_error(yte, model15.predict(X15_te)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25533f06",
      "metadata": {
        "id": "25533f06"
      },
      "source": [
        "\n",
        "**Task:** Discuss how complexity affects train vs test error.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 18: Cross-Validation for Degree Selection\n",
        "\n",
        "**Explanation:** Use CV to choose polynomial degree that balances bias and variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d601fbe4",
      "metadata": {
        "id": "d601fbe4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "degrees = range(1, 11)\n",
        "scores = []\n",
        "for d in degrees:\n",
        "    pipeline = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
        "    cv_score = np.mean(cross_val_score(pipeline, x_poly, y_poly, cv=5, scoring='r2'))\n",
        "    scores.append(cv_score)\n",
        "\n",
        "plt.plot(degrees, scores, marker='o')\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel('CV R²')\n",
        "plt.title('Cross-Validation Score by Polynomial Degree')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e3c4bf",
      "metadata": {
        "id": "82e3c4bf"
      },
      "source": [
        "\n",
        "**Task:** Identify the optimal degree and explain why.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 19: Bias-Variance Tradeoff Simulation\n",
        "\n",
        "**Explanation:** In this exercise, you will empirically observe the tradeoff between bias (error due to overly simple models) and variance (error due to overly complex models) as both model complexity and data noise change:\n",
        "\n",
        "*   Bias: Simpler models (lower-degree polynomials) may have high bias, systematically underfitting the true quadratic relationship and yielding large error even on training data.\n",
        "*   Variance: Complex models (higher-degree polynomials) are flexible and can fit noise in the training set, leading to low training error but high variance—tiny changes in the data (e.g., increased noise) will drastically change the fitted curve.\n",
        "\n",
        "By looping over different noise levels and polynomial degrees, you will see how the optimal complexity (the degree that minimizes training MSE) shifts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a4986b2",
      "metadata": {
        "id": "8a4986b2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "noise_levels = [0.5, 1, 2, 5]\n",
        "degrees = [1, 3, 5, 10]\n",
        "results = {}\n",
        "for noise in noise_levels:\n",
        "    xs = x_poly\n",
        "    ys = 1 - 2*xs + xs**2 + rng.randn(100,1)*noise\n",
        "    results[noise] = []\n",
        "    for d in degrees:\n",
        "        poly = PolynomialFeatures(d)\n",
        "        Xd = poly.fit_transform(xs)\n",
        "        model = LinearRegression().fit(Xd, ys)\n",
        "        results[noise].append(mean_squared_error(ys, model.predict(Xd)))\n",
        "\n",
        "for noise, errs in results.items():\n",
        "    plt.plot(degrees, errs, label=f'noise={noise}')\n",
        "plt.xlabel('Degree')\n",
        "plt.ylabel('Training MSE')\n",
        "plt.legend()\n",
        "plt.title('Effect of Noise and Complexity')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672b8e49",
      "metadata": {
        "id": "672b8e49"
      },
      "source": [
        "\n",
        "**Task:** Explain how noise level influences optimal complexity (degree or number of parameters).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 20: Decision Tree vs Polynomial\n",
        "\n",
        "**Explanation:** Compare flexible non-parametric model to parametric polynomials. We will study decision trees on Week 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0d35bb",
      "metadata": {
        "id": "6e0d35bb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Fit models\n",
        "poly2 = make_pipeline(PolynomialFeatures(2), LinearRegression()).fit(x_poly, y_poly)\n",
        "tree = DecisionTreeRegressor(max_depth=5).fit(x_poly, y_poly)\n",
        "\n",
        "# Predictions\n",
        "grid = np.linspace(-3,3,200).reshape(-1,1)\n",
        "p2 = poly2.predict(PolynomialFeatures(2).fit_transform(grid))\n",
        "tp = tree.predict(grid)\n",
        "\n",
        "plt.scatter(x_poly, y_poly, alpha=0.2)\n",
        "plt.plot(grid, p2, label='Poly deg2')\n",
        "plt.plot(grid, tp, label='Decision Tree')\n",
        "plt.legend()\n",
        "plt.title('Polynomial vs Tree Regression')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58eae2a2",
      "metadata": {
        "id": "58eae2a2"
      },
      "source": [
        "\n",
        "**Task:** Discuss model flexibility and overfitting tendencies.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 6: Generalization & Curse of Dimensionality\n",
        "\n",
        "### Exercise 21: Volume of High-Dimensional Shells\n",
        "\n",
        "**Explanation:**\n",
        "In low dimensions (e.g., a 2D circle or 3D sphere), a significant portion of the volume lies closer to the center. However, as dimensionality increases, almost all of the volume of a unit ball concentrates in a thin outer shell. This counterintuitive phenomenon underpins many challenges in high-dimensional learning:\n",
        "\n",
        "- Sparsity of Data: Points sampled uniformly become increasingly far apart, making local neighborhoods less meaningful.\n",
        "\n",
        "- Distance Concentration: Distances between points tend to converge, degrading the effectiveness of distance-based algorithms.\n",
        "\n",
        "- Sampling Difficulty: To cover a high-dimensional space adequately requires exponentially more samples.\n",
        "\n",
        "By empirically measuring the fraction of points that fall within the outer 1% of the radius for increasing dimensions, you will see how quickly the boundary dominates the volume. This illustrates why high-dimensional regression and clustering often require dimensionality reduction or strong regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67abe779",
      "metadata": {
        "id": "67abe779"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dims = [1,2,3,5,10,20]\n",
        "fractions = []\n",
        "for d in dims:\n",
        "    pts = rng.randn(100000, d)\n",
        "    pts /= np.linalg.norm(pts, axis=1)[:,None]\n",
        "    r = rng.rand(100000)**(1/d) # Lots of points\n",
        "    dist = r  # radial distance\n",
        "    shell = np.logical_and(dist >= 0.99, dist <= 1)\n",
        "    fractions.append(shell.mean())\n",
        "\n",
        "plt.plot(dims, fractions, marker='o')\n",
        "plt.xlabel('Dimension')\n",
        "plt.ylabel('Fraction in Outer 1% Shell')\n",
        "plt.title('Curse of Dimensionality')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ba5b22c",
      "metadata": {
        "id": "9ba5b22c"
      },
      "source": [
        "\n",
        "**Task:** Interpret why learning in high dimensions is challenging.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 22: Feature Correlation Heatmap\n",
        "\n",
        "**Explanation:** Visualize redundancy in high-dimensional Gaussian data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8782ec68",
      "metadata": {
        "id": "8782ec68"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X_hd = rng.randn(1000, 10)\n",
        "\n",
        "# Correlation matrix\n",
        "corr = np.corrcoef(X_hd, rowvar=False)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(corr, annot=True, fmt='.2f')\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26897f6e",
      "metadata": {
        "id": "26897f6e"
      },
      "source": [
        "\n",
        "**Task:** Identify any strongly correlated feature pairs and discuss implications.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 23: Nearest Neighbor Distance Ratio\n",
        "\n",
        "**Explanation:** (Advanced) Compare distance concentration in original vs random-projection spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3a0a61",
      "metadata": {
        "id": "eb3a0a61"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "# Original distances\n",
        "pt = rng.randn(1000,50)\n",
        "dists = np.linalg.norm(pt - pt[0], axis=1)\n",
        "\n",
        "# Project to 2D\n",
        "proj = GaussianRandomProjection(n_components=2).fit_transform(pt)\n",
        "dists2 = np.linalg.norm(proj - proj[0], axis=1)\n",
        "\n",
        "print('Avg dist in 50D:', dists.mean())\n",
        "print('Avg dist in 2D:', dists2.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b917c38d",
      "metadata": {
        "id": "b917c38d"
      },
      "source": [
        "\n",
        "**Task:** Discuss how dimensionality affects distance-based methods.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 7: Regularization\n",
        "\n",
        "### Exercise 24: Ridge vs Lasso Coefficient Paths\n",
        "\n",
        "**Explanation:** Observe how regularization shrinks coefficients as penalty increases. First, we will generate a synthetic dataset with multiple features and a noisy target, then fit Ridge and Lasso across a range of penalty strengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288dbfe0",
      "metadata": {
        "id": "288dbfe0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "# Generate synthetic data\n",
        "rng = np.random.RandomState(0)\n",
        "X = rng.randn(100, 10)                # 100 samples, 10 features\n",
        "true_coef = rng.randn(10)             # underlying coefficients\n",
        "y = X.dot(true_coef) + rng.randn(100) * 5  # add noise to targets\n",
        "\n",
        "# Regularization paths\n",
        "alphas = np.logspace(-3, 3, 50) # Fitting the model for different values of alpha\n",
        "coefs_ridge = []\n",
        "coefs_lasso = []\n",
        "for a in alphas:\n",
        "    coefs_ridge.append(Ridge(alpha=a).fit(X, y).coef_)\n",
        "    coefs_lasso.append(Lasso(alpha=a, max_iter=10000).fit(X, y).coef_)\n",
        "\n",
        "coefs_ridge = np.array(coefs_ridge)\n",
        "coefs_lasso = np.array(coefs_lasso)\n",
        "\n",
        "# Plot paths for each feature\n",
        "for i in range(X.shape[1]):\n",
        "    plt.plot(alphas, coefs_ridge[:, i], '--', label=f'Ridge coef {i}')\n",
        "    plt.plot(alphas, coefs_lasso[:, i], '-', label=f'Lasso coef {i}')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('Coefficient value')\n",
        "plt.title('Ridge and Lasso Regularization Paths')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c9245d0",
      "metadata": {
        "id": "8c9245d0"
      },
      "source": [
        "\n",
        "**Task:** Explain differences in how Ridge and Lasso treat coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 25: CV for Regularization Parameter\n",
        "\n",
        "**Explanation:** Tune penalty strength via grid search and cross-validation. Read the documentation on the unknown functions. We will see more about this in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c62ec9",
      "metadata": {
        "id": "87c62ec9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "param_grid = {'alpha': np.logspace(-3,3,20)}\n",
        "gs = GridSearchCV(Ridge(), param_grid, cv=5, scoring='r2')\n",
        "gs.fit(X, y)\n",
        "print('Best alpha:', gs.best_params_['alpha'])\n",
        "print('Best CV R²:', gs.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03569b69",
      "metadata": {
        "id": "03569b69"
      },
      "source": [
        "\n",
        "**Task:** Plot CV scores vs alpha to visualize the tuning curve.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 26: ElasticNet Comparison\n",
        "\n",
        "**Explanation:** (Advanced) Combine L1 and L2 penalties to handle correlated features. Read about ElasticNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b754522",
      "metadata": {
        "id": "2b754522"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'alpha': np.logspace(-3,3,10), 'l1_ratio': [0.1,0.5,0.9]}\n",
        "gs_en = GridSearchCV(ElasticNet(max_iter=10000), param_grid, cv=5)\n",
        "gs_en.fit(X, y)\n",
        "print('Best params:', gs_en.best_params_)\n",
        "print('Best CV R²:', gs_en.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc8e425",
      "metadata": {
        "id": "acc8e425"
      },
      "source": [
        "\n",
        "**Task:** Discuss scenarios where ElasticNet outperforms Ridge or Lasso alone.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 8: Loss Functions for Regression\n",
        "\n",
        "### Exercise 27: MSE vs MAE Loss Curves\n",
        "\n",
        "**Explanation:** Plot MSE and MAE as functions of prediction error to understand sensitivity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4c426c",
      "metadata": {
        "id": "0a4c426c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "e = np.linspace(-5,5,400)\n",
        "mse = e**2\n",
        "mae = np.abs(e)\n",
        "\n",
        "plt.plot(e, mse, label='MSE')\n",
        "plt.plot(e, mae, label='MAE')\n",
        "plt.xlabel('Error')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Functions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b71ea02",
      "metadata": {
        "id": "2b71ea02"
      },
      "source": [
        "\n",
        "**Task:** Identify ranges where MAE and MSE differ most and explain why.\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 28: Huber Loss Implementation\n",
        "\n",
        "**Explanation:** Huber loss combines MSE and MAE to be robust to outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5382fe2c",
      "metadata": {
        "id": "5382fe2c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def huber(e, delta=1.0):\n",
        "    return np.where(np.abs(e) <= delta, 0.5*e**2, delta*(np.abs(e) - 0.5*delta))\n",
        "\n",
        "e = np.linspace(-5,5,400)\n",
        "\n",
        "plt.plot(e, e**2, '--', label='MSE')\n",
        "plt.plot(e, np.abs(e), ':', label='MAE')\n",
        "plt.plot(e, huber(e), '-', label='Huber')\n",
        "plt.xlabel('Error')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Comparison of Loss Functions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa8f572a",
      "metadata": {
        "id": "aa8f572a"
      },
      "source": [
        "\n",
        "**Task:** Explain how Huber loss mitigates outlier influence compared to MSE.\n",
        "\n",
        "---\n",
        "\n",
        "## Section 9: Error Analysis & Visualization\n",
        "\n",
        "### Exercise 29: Residuals vs Predictions\n",
        "\n",
        "**Explanation:** Residual plots help detect non-linearity, [heteroscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity), and bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad39ef9a",
      "metadata": {
        "id": "ad39ef9a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "rng = np.random.RandomState(42) # Fix the seed for reproducibility\n",
        "X = rng.uniform(-5, 5, size=(200, 1))\n",
        "y = 2.0 * X.flatten() + 1.0 + rng.randn(200) * 2.5  # true line with noise\n",
        "\n",
        "# Fit linear model\n",
        "model = LinearRegression().fit(X, y)\n",
        "preds = model.predict(X)\n",
        "residuals = preds - y\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(preds, residuals, alpha=0.6)\n",
        "plt.axhline(0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c789b1af",
      "metadata": {
        "id": "c789b1af"
      },
      "source": [
        "\n",
        "**Task:** Look for patterns indicating model inadequacy (e.g., funnel shapes).\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise 30: Learning Curves\n",
        "\n",
        "**Explanation:** Use learning curves to determine if more data or a different model complexity would help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c926d1a",
      "metadata": {
        "id": "4c926d1a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    LinearRegression(), X, y, cv=5, scoring='r2',\n",
        "    train_sizes=np.linspace(0.1,1.0,10))\n",
        "\n",
        "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Train')\n",
        "plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f714f7b",
      "metadata": {
        "id": "6f714f7b"
      },
      "source": [
        "\n",
        "**Task:** Determine whether gathering more data or increasing model complexity is likely to improve performance.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}